{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch as torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import  numpy\n",
    "im = Image.open('Images/MIRI/Cartwheel Galaxy.tif')\n",
    "im.show()\n",
    "# imarray = numpy.array(im)\n",
    "# imarray.shape\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image_data):\n",
    "    fig, axs = plt.subplots(1,len(image_data),figsize=(16,8))\n",
    "    for x, axes in zip(image_data, axs):\n",
    "        axes.imshow(Image.open(image_data[x]))\n",
    "        \n",
    "    \n",
    "    #Displaying Binarized images\n",
    "    figure, axiss = plt.subplots(1,len(image_data),figsize=(16,8))\n",
    "    for i in range(len(image_data)):\n",
    "        images=imread(image_data[i])\n",
    "        grey_images=rgb2gray(images)\n",
    "        thresh=threshold_otsu(grey_images)\n",
    "        binary=grey_images>thresh\n",
    "        axiss[i].imshow(binary,cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.morphology import area_closing, area_opening\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "\n",
    "#Displaying Images normally\n",
    "def show_image(image_data):\n",
    "    fig, axs = plt.subplots(1,len(image_data),figsize=(16,8))\n",
    "    for x, axes in zip(image_data, axs):\n",
    "        axes.imshow(imread(x))\n",
    "    \n",
    "    #Displaying Binarized images\n",
    "    figure, axiss = plt.subplots(1,len(image_data),figsize=(16,8))\n",
    "    for i in range(len(image_data)):\n",
    "        images=imread(image_data[i])\n",
    "        grey_images=rgb2gray(images)\n",
    "        thresh=threshold_otsu(grey_images)\n",
    "        binary=grey_images>thresh\n",
    "        axiss[i].imshow(binary,cmap=\"gray\")\n",
    "\n",
    "#Paths are hard coded for better understanding\n",
    "#Change paths here if the images are in a different folder\n",
    "MIRI=[\"Images/MIRI/Cartwheel Galaxy.tif\",\"Images/MIRI/Stephan Quintet.tif\"]\n",
    "MIRI_IMAGES=show_image(MIRI)\n",
    "NIRCam=[\"Images/NIRCam/Cosmic Cliffs in the Carina Nebula.tif\",\"Images/NIRCam/Webb First Deep Field.tif\"]\n",
    "NIRCam_IMAGES=show_image(NIRCam)\n",
    "COMPOSITE=[\"Images/NIRCam and MIRI Composite/Cosmic Cliffs in the Carina Nebula.tif\",\"Images/NIRCam and MIRI Composite/Stephan Quintet.tif\"]\n",
    "COMPOSITE_IMAGES=show_image(COMPOSITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "dataset = pd.read_csv('Data.csv')\n",
    "dataset.head()\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop('Unnamed: 0' , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[['area', 'convex_area', 'bbox_area', 'major_axis_length',\n",
    "       'minor_axis_length', 'perimeter', 'equivalent_diameter',\n",
    "       'mean_intensity', 'solidity', 'eccentricity']]\n",
    "Y = dataset['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing test-train 80-20\n",
    "indices = np.arange(len(dataset) , dtype = np.int64)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:int(len(dataset)*0.8)].tolist()\n",
    "test_indices = indices[int(len(dataset)*0.8):].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.values[train_indices]\n",
    "Y_train = Y.values[train_indices]\n",
    "X_test = X.values[test_indices]\n",
    "Y_test = Y.values[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# creating the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# training the model\n",
    "model.fit(X_train , Y_train)\n",
    "\n",
    "# predicting the model\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# calculating the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test , Y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "my_model = XGBRegressor()\n",
    "my_model.fit(X_train, Y_train)\n",
    "Y_pred = my_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "# Create adaboost classifer object\n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=0)\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "model1 = abc.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier)\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, roc_auc_score\n",
    "\n",
    "\n",
    "knn=KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, Y_train)\n",
    "y_pred=knn.predict(X_test)\n",
    "\n",
    "knn_acc=round(knn.score(X_train, Y_train)*100, 2)\n",
    "knn_acc_test = round(accuracy_score(Y_test, y_pred)*100,2)\n",
    "print(f'Train Accuracy Score of Basic KNN model: % {knn_acc}')\n",
    "print(f'Test Accuracy Score of Basic KNN model: % {knn_acc_test}')\n",
    "\n",
    "# Get precision, recall, f1 scores\n",
    "precision, recall, f1score, support = score(Y_test, y_pred, average='macro')\n",
    "print(f'Precision Score : {precision}')\n",
    "print(f'Recall Score : {recall}')\n",
    "print(f'F1-score Score : {f1score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_list = []\n",
    "# to contain model names\n",
    "models = []\n",
    "evaluation_list.append(dict([\n",
    "    ('Model', 'KNN'),\n",
    "    ('Train Accuracy', round(knn_acc, 2)),\n",
    "    ('Test Accuracy', round(knn_acc_test, 2)),\n",
    "    ('Precision', round(precision, 2)),\n",
    "    ('Recall', round(recall, 2)),\n",
    "    ('F1', round(f1score, 2))\n",
    "     ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg=LogisticRegression()\n",
    "log_reg.fit(X_train, Y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "# Get probabilities\n",
    "#pred_proba = log_reg.predict_proba(X_test)\n",
    "\n",
    "lr_acc=round(log_reg.score(X_train, Y_train) * 100, 2)\n",
    "lr_acc_test=round(accuracy_score(Y_test, y_pred) * 100, 2)\n",
    "\n",
    "print(f'Train Accuracy Score of Basic Logistic Reg.: % {lr_acc}')\n",
    "print(f'Test Accuracy Score of Basic Logistic Reg.: % {lr_acc_test}')\n",
    "\n",
    "# Get precision, recall, f1 scores\n",
    "precision, recall, f1score, support = score(Y_test, y_pred, average='macro')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "evaluation_list.append(dict([\n",
    "    ('Model', 'Logistic Regression'),\n",
    "    ('Train Accuracy', round(lr_acc, 2)),\n",
    "    ('Test Accuracy', round(lr_acc_test, 2)),\n",
    "    ('Precision', round(precision, 2)),\n",
    "    ('Recall', round(recall, 2)),\n",
    "    ('F1', round(f1score, 2))\n",
    "     ]))\n",
    "\n",
    "nb_g=GaussianNB()\n",
    "nb_g.fit(X_train, Y_train)\n",
    "y_pred = nb_g.predict(X_test)\n",
    "\n",
    "nbg_acc=round(nb_g.score(X_train, Y_train) * 100, 2)\n",
    "nbg_acc_test=round(accuracy_score(Y_test, y_pred) * 100, 2)\n",
    "\n",
    "print(f'Train Accuracy Score of GaussianNB: % {nbg_acc}')\n",
    "print(f'Test Accuracy Score of GaussianNB: % {nbg_acc_test}')\n",
    "\n",
    "# Get precision, recall, f1 scores\n",
    "precision, recall, f1score, support = score(Y_test, y_pred, average='macro')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "\n",
    "evaluation_list.append(dict([\n",
    "    ('Model', 'GaussianNB'),\n",
    "    ('Train Accuracy', round(nbg_acc, 2)),\n",
    "    ('Test Accuracy', round(nbg_acc_test, 2)),\n",
    "    ('Precision', round(precision, 2)),\n",
    "    ('Recall', round(recall, 2)),\n",
    "    ('F1', round(f1score, 2))\n",
    "     ]))\n",
    "\n",
    "nb_b=BernoulliNB()\n",
    "nb_b.fit(X_train, Y_train)\n",
    "y_pred = nb_b.predict(X_test)\n",
    "\n",
    "nbb_acc=round(nb_b.score(X_train, Y_train) * 100, 2)\n",
    "nbb_acc_test=round(accuracy_score(Y_test, y_pred) * 100, 2)\n",
    "\n",
    "print(f'Train Accuracy Score of BernoulliNB: % {nbb_acc}')\n",
    "print(f'Test Accuracy Score of BernoulliNB: % {nbb_acc_test}')\n",
    "\n",
    "# Get precision, recall, f1 scores\n",
    "precision, recall, f1score, support = score(Y_test, y_pred, average='macro')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "\n",
    "evaluation_list.append(dict([\n",
    "    ('Model', 'BernoulliNB'),\n",
    "    ('Train Accuracy', round(nbb_acc, 2)),\n",
    "    ('Test Accuracy', round(nbb_acc_test, 2)),\n",
    "    ('Precision', round(precision, 2)),\n",
    "    ('Recall', round(recall, 2)),\n",
    "    ('F1', round(f1score, 2))\n",
    "     ]))\n",
    "\n",
    "linear_svc = LinearSVC()\n",
    "y_pred = linear_svc.fit(X_train, Y_train).predict(X_test)\n",
    "\n",
    "linear_svc_acc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n",
    "linear_svc_acc_test=round(accuracy_score(Y_test, y_pred) * 100, 2)\n",
    "print(f'Train Accuracy Score of SVC model: % {linear_svc_acc}')\n",
    "print(f'Test Accuracy Score of SVC model: % {linear_svc_acc_test}')\n",
    "\n",
    "# Get precision, recall, f1 scores\n",
    "precision, recall, f1score, support = score(Y_test, y_pred, average='macro')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "\n",
    "evaluation_list.append(dict([\n",
    "    ('Model', 'Linear SVC'),\n",
    "    ('Train Accuracy', round(linear_svc_acc, 2)),\n",
    "    ('Test Accuracy', round(linear_svc_acc_test, 2)),\n",
    "    ('Precision', round(precision, 2)),\n",
    "    ('Recall', round(recall, 2)),\n",
    "    ('F1', round(f1score, 2))\n",
    "     ]))\n",
    "\n",
    "dtree=DecisionTreeClassifier()\n",
    "dtree.fit(X_train, Y_train)\n",
    "y_pred = dtree.predict(X_test)\n",
    "\n",
    "dt_acc = round(dtree.score(X_train, Y_train) * 100, 2)\n",
    "dt_acc_test = round(accuracy_score(Y_test, y_pred) *100 ,2)\n",
    "\n",
    "print(f'Train Accuracy Score of Decision Tree: % {dt_acc}')\n",
    "print(f'Test Accuracy Score of Decision Tree: % {dt_acc_test}')\n",
    "# Get precision, recall, f1 scores\n",
    "precision, recall, f1score, support = score(Y_test, y_pred, average='macro')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "\n",
    "evaluation_list.append(dict([\n",
    "    ('Model', 'Decision Tree'),\n",
    "    ('Train Accuracy', round(dt_acc, 2)),\n",
    "    ('Test Accuracy', round(dt_acc_test, 2)),\n",
    "    ('Precision', round(precision, 2)),\n",
    "    ('Recall', round(recall, 2)),\n",
    "    ('F1', round(f1score, 2))\n",
    "     ]))\n",
    "\n",
    "num_estimator = 100\n",
    "seed = 7\n",
    "bag = BaggingClassifier(base_estimator=dtree, n_estimators=num_estimator,\n",
    "    bootstrap=True, n_jobs=-1, random_state=seed)\n",
    "\n",
    "bag.fit(X_train, Y_train)\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "bag_acc = round(bag.score(X_train, Y_train) * 100, 2)\n",
    "bag_acc_test = round(accuracy_score(Y_test, y_pred) * 100, 2)\n",
    "\n",
    "print(f'Train Accuracy Score of Bagged Decision Trees: % {bag_acc}')\n",
    "print(f'Test Accuracy Score of Bagged Decision Trees: % {bag_acc_test}')\n",
    "\n",
    "# Get precision, recall, f1 scores\n",
    "precision, recall, f1score, support = score(Y_test, y_pred, average='macro')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "\n",
    "evaluation_list.append(dict([\n",
    "    ('Model', 'Bagging Classifier'),\n",
    "    ('Train Accuracy', round(bag_acc, 2)),\n",
    "    ('Test Accuracy', round(bag_acc_test, 2)),\n",
    "    ('Precision', round(precision, 2)),\n",
    "    ('Recall', round(recall, 2)),\n",
    "    ('F1', round(f1score, 2))\n",
    "     ]))\n",
    "\n",
    "rforest = RandomForestClassifier(n_estimators = num_estimator)\n",
    "rforest.fit(X_train, Y_train)\n",
    "y_pred = rforest.predict(X_test)\n",
    "\n",
    "rf_acc = round(rforest.score(X_train, Y_train) * 100, 2)\n",
    "rf_acc_test = round(accuracy_score(Y_test, y_pred) * 100, 2)\n",
    "\n",
    "print(f'Train Accuracy Score of Random Forest: % {rf_acc}')\n",
    "print(f'Test Accuracy Score of Random Forest: % {rf_acc_test}')\n",
    "# Get precision, recall, f1 scores\n",
    "precision, recall, f1score, support = score(Y_test, y_pred, average='macro')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "\n",
    "evaluation_list.append(dict([\n",
    "    ('Model', 'Random Forest'),\n",
    "    ('Train Accuracy', round(rf_acc, 2)),\n",
    "    ('Test Accuracy', round(rf_acc_test, 2)),\n",
    "    ('Precision', round(precision, 2)),\n",
    "    ('Recall', round(recall, 2)),\n",
    "    ('F1', round(f1score, 2))\n",
    "     ]))\n",
    "\n",
    "etc = ExtraTreesClassifier(n_estimators=num_estimator)\n",
    "etc.fit(X_train, Y_train)\n",
    "y_pred = etc.predict(X_test)\n",
    "\n",
    "etc_acc = round(etc.score(X_train, Y_train) * 100, 2)\n",
    "etc_acc_test = round(accuracy_score(Y_test, y_pred) * 100, 2)\n",
    "\n",
    "print(f'Train Accuracy Score of Extra Trees: % {etc_acc}')\n",
    "print(f'Test Accuracy Score of Extra Trees: % {etc_acc_test}')\n",
    "\n",
    "# Get precision, recall, f1 scores\n",
    "precision, recall, f1score, support = score(Y_test, y_pred, average='macro')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "\n",
    "evaluation_list.append(dict([\n",
    "    ('Model', 'Extra Trees'),\n",
    "    ('Train Accuracy', round(etc_acc, 2)),\n",
    "    ('Test Accuracy', round(etc_acc_test, 2)),\n",
    "    ('Precision', round(precision, 2)),\n",
    "    ('Recall', round(recall, 2)),\n",
    "    ('F1', round(f1score, 2))\n",
    "     ]))\n",
    "\n",
    "abc = AdaBoostClassifier(base_estimator=dtree, n_estimators=num_estimator, random_state = seed)\n",
    "abc.fit(X_train, Y_train)\n",
    "y_pred = abc.predict(X_test)\n",
    "\n",
    "abc_acc = round(abc.score(X_train, Y_train) *100 , 2)\n",
    "abc_acc_test = round(accuracy_score(Y_test, y_pred) * 100 ,2)\n",
    "\n",
    "print(f'Train Accuracy Score of AdaBoostClassifier: % {abc_acc}')\n",
    "print(f'Test Accuracy Score of AdaBoostClassifier: % {abc_acc_test}')\n",
    "\n",
    "# Get precision, recall, f1 scores\n",
    "precision, recall, f1score, support = score(Y_test, y_pred, average='macro')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "\n",
    "evaluation_list.append(dict([\n",
    "    ('Model', 'AdaBoost'),\n",
    "    ('Train Accuracy', round(abc_acc, 2)),\n",
    "    ('Test Accuracy', round(abc_acc_test, 2)),\n",
    "    ('Precision', round(precision, 2)),\n",
    "    ('Recall', round(recall, 2)),\n",
    "    ('F1', round(f1score, 2))\n",
    "     ]))\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=num_estimator, learning_rate=0.01, random_state=seed)\n",
    "gbc.fit(X_train, Y_train)\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "gbc_acc = round(gbc.score(X_train, Y_train) * 100, 2)\n",
    "gbc_acc_test = round(accuracy_score(Y_test, y_pred) * 100, 2)\n",
    "\n",
    "print(f'Train Accuracy Score of GradientBoostingClassifier: % {gbc_acc}')\n",
    "print(f'Test Accuracy Score of GradientBoostingClassifier: % {gbc_acc_test}')\n",
    "\n",
    "# Get precision, recall, f1 scores\n",
    "precision, recall, f1score, support = score(Y_test, y_pred, average='macro')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "\n",
    "evaluation_list.append(dict([\n",
    "    ('Model', 'GradientBoosting'),\n",
    "    ('Train Accuracy', round(gbc_acc, 2)),\n",
    "    ('Test Accuracy', round(gbc_acc_test, 2)),\n",
    "    ('Precision', round(precision, 2)),\n",
    "    ('Recall', round(recall, 2)),\n",
    "    ('F1', round(f1score, 2))\n",
    "     ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier(\n",
    "    estimators=[('lr', log_reg), ('nb',nb_g), ('rf', rforest), ('gbc', gbc)],\n",
    "    voting = 'soft')\n",
    "\n",
    "vc.fit(X_train, Y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "vc_acc = round(vc.score(X_train, Y_train) * 100, 2)\n",
    "vc_acc_test = round(accuracy_score(Y_test, y_pred) * 100, 2)\n",
    "\n",
    "print(f'Train Accuracy Score of Soft VotingClassifier: % {vc_acc}')\n",
    "print(f'Test Accuracy Score of Soft VotingClassifier: % {vc_acc_test}')\n",
    "\n",
    "# Get precision, recall, f1 scores\n",
    "precision, recall, f1score, support = score(Y_test, y_pred, average='macro')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "\n",
    "evaluation_list.append(dict([\n",
    "    ('Model', 'Soft VotingClassifier'),\n",
    "    ('Train Accuracy', round(vc_acc, 2)),\n",
    "    ('Test Accuracy', round(vc_acc_test, 2)),\n",
    "    ('Precision', round(precision, 2)),\n",
    "    ('Recall', round(recall, 2)),\n",
    "    ('F1', round(f1score, 2))\n",
    "     ]))\n",
    "\n",
    "vc_hard = VotingClassifier(\n",
    "    estimators=[('lr', log_reg), ('nb',nb_g), ('rf', rforest), ('gbc', gbc)],\n",
    "    voting = 'hard')\n",
    "\n",
    "vc_hard.fit(X_train, Y_train)\n",
    "y_pred = vc_hard.predict(X_test)\n",
    "\n",
    "vch_acc = round(vc_hard.score(X_train, Y_train) * 100, 2)\n",
    "vch_acc_test = round(accuracy_score(Y_test, y_pred) * 100, 2)\n",
    "\n",
    "print(f'Train Accuracy Score of Hard VotingClassifier: % {vch_acc}')\n",
    "print(f'Test Accuracy Score of Hard VotingClassifier: % {vch_acc_test}')\n",
    "\n",
    "# Get precision, recall, f1 scores\n",
    "precision, recall, f1score, support = score(Y_test, y_pred, average='macro')\n",
    "print(f'Precision : {precision}')\n",
    "print(f'Recall    : {recall}')\n",
    "print(f'F1-score   : {f1score}')\n",
    "\n",
    "evaluation_list.append(dict([\n",
    "    ('Model', 'Hard VotingClassifier'),\n",
    "    ('Train Accuracy', round(vch_acc, 2)),\n",
    "    ('Test Accuracy', round(vch_acc_test, 2)),\n",
    "    ('Precision', round(precision, 2)),\n",
    "    ('Recall', round(recall, 2)),\n",
    "    ('F1', round(f1score, 2))\n",
    "     ]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
